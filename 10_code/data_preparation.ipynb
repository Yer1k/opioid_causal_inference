{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Notebook\n",
    "\n",
    "#### This notebook will be the test bed for data read functions to ingest data from a data folder on the local machine\n",
    "\n",
    "#### The end outputs of this notebook are that all data structures will have a 'COUNTY NAME, ST' column, and where applicable a FIPS code as well. Merges will be performed elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set the states concerned for the analysis\n",
    "states = [\"FL\", \"TX\", \"WA\", \"OR\", \"AL\", \"OK\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the state abbreviations as its own DF\n",
    "state_df = pd.read_table(\"../00_source_data/03_state_names.rtf\", sep=\",\")\n",
    "state_df.columns = [\"STATE\", \"ABBREV\"]\n",
    "\n",
    "# make state upper\n",
    "state_df[\"STATE\"] = state_df[\"STATE\"].str.upper()\n",
    "\n",
    "# drop the trailing slash from the abbrev\n",
    "state_df[\"ABBREV\"] = state_df[\"ABBREV\"].str[0:2]\n",
    "# state_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAPO Dataset\n",
    "\n",
    "This section takes an argument for the path to the WAPO dataset and will ultimately return an annualized dataframe of the states with respective values for each year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ingest actions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to WAPO file\n",
    "wapo = \"/Users/andrewkroening/Desktop/720_Data/arcos_all_washpost.tsv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the WAPO file as chunks\n",
    "\n",
    "chunks = 500000  # Leave this, there will be 358 chunks\n",
    "\n",
    "chunk_counter = 0\n",
    "\n",
    "wapo_df = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    wapo,\n",
    "    sep=\"\\t\",\n",
    "    compression=\"gzip\",\n",
    "    chunksize=chunks,\n",
    "    usecols=[\n",
    "        \"BUYER_COUNTY\",\n",
    "        \"BUYER_STATE\",\n",
    "        \"DRUG_NAME\",\n",
    "        \"TRANSACTION_DATE\",\n",
    "        \"QUANTITY\",\n",
    "        \"UNIT\",\n",
    "    ],\n",
    "):\n",
    "    chunk_counter += 1\n",
    "    percent_chunk = round(chunk_counter / 358 * 100, 2)\n",
    "    print(\"Reading chunk: \", chunk_counter, \"of 358 (\", percent_chunk, \"%)\")\n",
    "\n",
    "    # filter the chunk to only include the stations in the list\n",
    "    chunk = chunk[chunk[\"BUYER_STATE\"].isin(states)].copy()\n",
    "\n",
    "    chunk[\"TRANSACTION_DATE\"] = pd.to_datetime(\n",
    "        chunk[\"TRANSACTION_DATE\"], format=\"%m%d%Y\"\n",
    "    )\n",
    "\n",
    "    chunk[\"YEAR\"] = chunk[\"TRANSACTION_DATE\"].dt.year\n",
    "    chunk[\"MONTH\"] = chunk[\"TRANSACTION_DATE\"].dt.month\n",
    "\n",
    "    chunk = chunk.groupby([\"BUYER_COUNTY\", \"BUYER_STATE\", \"YEAR\", \"MONTH\"]).agg(\n",
    "        {\"QUANTITY\": \"sum\"}\n",
    "    )\n",
    "\n",
    "    # concat with the base df\n",
    "    wapo_df = pd.concat([wapo_df, chunk])\n",
    "\n",
    "print(\"Ingest complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the wapo df\n",
    "print(\"Adding county name...\")\n",
    "\n",
    "# Add an index to the wapo_df\n",
    "wapo_df = wapo_df.reset_index()\n",
    "\n",
    "# Do some transformations on the WAPO dataset\n",
    "wapo_df[\"COUNTY_NAME\"] = wapo_df[\"BUYER_COUNTY\"] + \" COUNTY, \" + wapo_df[\"BUYER_STATE\"]\n",
    "\n",
    "print(\"Grouping WAPO data...\")\n",
    "\n",
    "wapo_df = (\n",
    "    wapo_df.groupby([\"COUNTY_NAME\", \"BUYER_STATE\", \"YEAR\", \"MONTH\"])\n",
    "    .agg({\"QUANTITY\": \"sum\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# # rename buyer state to state\n",
    "wapo_df = wapo_df.rename(columns={\"BUYER_STATE\": \"STATE\"})\n",
    "\n",
    "# # Change the year column to a datetime object\n",
    "wapo_df[\"YEAR\"] = pd.to_datetime(wapo_df[\"YEAR\"], format=\"%Y\")\n",
    "\n",
    "print(\"Operation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Print a sample to make sure you did it right***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(wapo_df.sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assert tests to verify that we have the right states, and some checks on the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that no counties are missing\n",
    "assert wapo_df[\"COUNTY_NAME\"].isnull().sum() == 0\n",
    "# Assert that states are in the list\n",
    "assert set(wapo_df[\"STATE\"].unique()) == set(states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Save the output file to a csv in the intermediate files directory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this file as a csv called wapo_clean.csv in the current directory\n",
    "wapo_df.to_csv(\"../20_intermediate_files/wapo_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitality Data\n",
    "\n",
    "This section takes the txt files passed and will return a dataframe with the respective values for each year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ingest actions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory path to find the txt files\n",
    "nick_path = \"../00_source_data/US_VitalStatistics/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting text files...\n",
      "Transforming vitality data...\n",
      "Operation complete\n"
     ]
    }
   ],
   "source": [
    "# generate a df from the txt files in a folder path\n",
    "\n",
    "# initialize the empty df\n",
    "nick_df = pd.DataFrame()\n",
    "\n",
    "# set a loop to iterate through the files in the folder\n",
    "print(\"Ingesting text files...\")\n",
    "\n",
    "# set a loop to iterate through the files in the folder\n",
    "for file in os.listdir(nick_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        txt_table = pd.read_table(\n",
    "            os.path.join(nick_path, file), sep=\"\\t\", skipfooter=15, engine=\"python\"\n",
    "        )\n",
    "        # remove the bottom rows that are not needed\n",
    "        # txt_table = txt_table.iloc[0:-16, :]\n",
    "        nick_df = pd.concat([nick_df, txt_table], axis=0)\n",
    "\n",
    "# subset to the columns we want\n",
    "vital_df = nick_df[\n",
    "    [\n",
    "        \"County\",\n",
    "        \"County Code\",\n",
    "        \"Year\",\n",
    "        \"Drug/Alcohol Induced Cause\",\n",
    "        \"Drug/Alcohol Induced Cause Code\",\n",
    "        \"Deaths\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "print(\"Transforming vitality data...\")\n",
    "\n",
    "# change the year column to a date time object with a year only\n",
    "vital_df[\"Year\"] = pd.to_datetime(vital_df[\"Year\"], format=\"%Y\")\n",
    "\n",
    "# change the County Code column to 6-digit string FIPS code\n",
    "vital_df[\"County Code\"] = vital_df[\"County Code\"].astype(str).str.zfill(6)\n",
    "\n",
    "# add a state column with the last two characters from county\n",
    "vital_df[\"State\"] = vital_df[\"County\"].str[-2:]\n",
    "\n",
    "# convert NaN deaths to 0\n",
    "vital_df[\"Deaths\"] = vital_df[\"Deaths\"].fillna(0)\n",
    "\n",
    "# change the county name to all caps\n",
    "vital_df[\"County\"] = vital_df[\"County\"].str.upper()\n",
    "\n",
    "# change all the column names to all caps\n",
    "vital_df.columns = vital_df.columns.str.upper()\n",
    "\n",
    "# rename county code to FIPS code\n",
    "vital_df = vital_df.rename(columns={\"COUNTY CODE\": \"FIPS\"})\n",
    "vital_df = vital_df.rename(columns={\"COUNTY\": \"COUNTY_NAME\"})\n",
    "\n",
    "# filter the vital df to the states of interest\n",
    "vital_df = vital_df[vital_df[\"STATE\"].isin(states)].copy()\n",
    "\n",
    "print(\"Operation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Subset to rows with drug deaths, group and add***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "All other non-drug and non-alcohol causes             6925\n",
       "Drug poisonings (overdose) Unintentional (X40-X44)    1484\n",
       "All other alcohol-induced causes                      1386\n",
       "Drug poisonings (overdose) Suicide (X60-X64)           385\n",
       "All other drug-induced causes                          129\n",
       "Drug poisonings (overdose) Undetermined (Y10-Y14)       70\n",
       "Alcohol poisonings (overdose) (X45, X65, Y15)           54\n",
       "Name: DRUG/ALCOHOL INDUCED CAUSE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital_df[\"DRUG/ALCOHOL INDUCED CAUSE\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Check a sample to inspect data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTY_NAME</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>DRUG/ALCOHOL INDUCED CAUSE</th>\n",
       "      <th>DRUG/ALCOHOL INDUCED CAUSE CODE</th>\n",
       "      <th>DEATHS</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>CITRUS COUNTY, FL</td>\n",
       "      <td>012017</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>All other alcohol-induced causes</td>\n",
       "      <td>A9</td>\n",
       "      <td>24</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>YOUNG COUNTY, TX</td>\n",
       "      <td>048503</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>233</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>LAKE COUNTY, OR</td>\n",
       "      <td>041037</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>98.0</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>WALLER COUNTY, TX</td>\n",
       "      <td>048473</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>296.0</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>CURRY COUNTY, OR</td>\n",
       "      <td>041015</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>379.0</td>\n",
       "      <td>OR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>KITSAP COUNTY, WA</td>\n",
       "      <td>053035</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Drug poisonings (overdose) Unintentional (X40-...</td>\n",
       "      <td>D1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>KENDALL COUNTY, TX</td>\n",
       "      <td>048259</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>235.0</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>TALLADEGA COUNTY, AL</td>\n",
       "      <td>001121</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>UPTON COUNTY, TX</td>\n",
       "      <td>048461</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LEE COUNTY, AL</td>\n",
       "      <td>001081</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>All other non-drug and non-alcohol causes</td>\n",
       "      <td>O9</td>\n",
       "      <td>781.0</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               COUNTY_NAME    FIPS       YEAR  \\\n",
       "617      CITRUS COUNTY, FL  012017 2015-01-01   \n",
       "4220      YOUNG COUNTY, TX  048503 2015-01-01   \n",
       "3079       LAKE COUNTY, OR  041037 2007-01-01   \n",
       "4098     WALLER COUNTY, TX  048473 2014-01-01   \n",
       "3267      CURRY COUNTY, OR  041015 2013-01-01   \n",
       "3985     KITSAP COUNTY, WA  053035 2005-01-01   \n",
       "3523    KENDALL COUNTY, TX  048259 2004-01-01   \n",
       "83    TALLADEGA COUNTY, AL  001121 2013-01-01   \n",
       "3712      UPTON COUNTY, TX  048461 2005-01-01   \n",
       "46          LEE COUNTY, AL  001081 2006-01-01   \n",
       "\n",
       "                             DRUG/ALCOHOL INDUCED CAUSE  \\\n",
       "617                    All other alcohol-induced causes   \n",
       "4220          All other non-drug and non-alcohol causes   \n",
       "3079          All other non-drug and non-alcohol causes   \n",
       "4098          All other non-drug and non-alcohol causes   \n",
       "3267          All other non-drug and non-alcohol causes   \n",
       "3985  Drug poisonings (overdose) Unintentional (X40-...   \n",
       "3523          All other non-drug and non-alcohol causes   \n",
       "83            All other non-drug and non-alcohol causes   \n",
       "3712          All other non-drug and non-alcohol causes   \n",
       "46            All other non-drug and non-alcohol causes   \n",
       "\n",
       "     DRUG/ALCOHOL INDUCED CAUSE CODE  DEATHS STATE  \n",
       "617                               A9      24    FL  \n",
       "4220                              O9     233    TX  \n",
       "3079                              O9    98.0    OR  \n",
       "4098                              O9   296.0    TX  \n",
       "3267                              O9   379.0    OR  \n",
       "3985                              D1    16.0    WA  \n",
       "3523                              O9   235.0    TX  \n",
       "83                                O9  1017.0    AL  \n",
       "3712                              O9    23.0    TX  \n",
       "46                                O9   781.0    AL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vital_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assert tests to verify that we have the right states, and some checks on the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vital_df.all().isnull().sum() == 0\n",
    "# Assert that states are in the list\n",
    "assert set(vital_df[\"STATE\"].unique()) == set(states)\n",
    "# Still need to find a way to check if Vital is correctly chunked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Save the output dataframe to the intermediate files directory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_df.to_csv(\"../20_intermediate_files/vital_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add FIPS Codes to the available data\n",
    "\n",
    "This chunk takes the path to FIPS codes and will ingest them into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ingest data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_path = \"../00_source_data/02_fcc_fips_codes.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***start with counties***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the fips codes\n",
    "fips_county_df = pd.read_table(fips_path, sep=\"\\t\", skiprows=71, header=\"infer\")\n",
    "\n",
    "# name the column header\n",
    "fips_county_df.columns = [\"FIPS\"]\n",
    "\n",
    "# split the FIPS column into two columns after the first 5 characters\n",
    "fips_county_df[\"COUNTY_NAME\"] = fips_county_df[\"FIPS\"].str[10:]\n",
    "\n",
    "# Turn the FIPS column into just the numbers\n",
    "fips_county_df[\"FIPS\"] = fips_county_df[\"FIPS\"].str[4:10]\n",
    "\n",
    "# remove the spaces from the county name\n",
    "fips_county_df[\"COUNTY_NAME\"] = fips_county_df[\"COUNTY_NAME\"].str.strip()\n",
    "\n",
    "# make county name upper\n",
    "fips_county_df[\"COUNTY_NAME\"] = fips_county_df[\"COUNTY_NAME\"].str.upper()\n",
    "\n",
    "print(\"Operation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the fips codes\n",
    "fips_state_df = pd.read_table(fips_path, sep=\"\\t\", skiprows=15, header=\"infer\")\n",
    "\n",
    "# keep the first 50 rows\n",
    "fips_state_df = fips_state_df.iloc[0:50, :]\n",
    "\n",
    "# name the column header\n",
    "fips_state_df.columns = [\"FIPS\"]\n",
    "\n",
    "# # split the FIPS column into two columns after the first 5 characters\n",
    "fips_state_df[\"STATE\"] = fips_state_df[\"FIPS\"].str[10:]\n",
    "fips_state_df[\"STATE\"] = fips_state_df[\"STATE\"].str.strip()\n",
    "\n",
    "# #Turn the FIPS column into just the numbers\n",
    "fips_state_df[\"FIPS\"] = fips_state_df[\"FIPS\"].str[4:10]\n",
    "\n",
    "# merge to add the state abbreviations from the state_df\n",
    "fips_state_df = fips_state_df.merge(state_df, on=\"STATE\", how=\"left\")\n",
    "\n",
    "print(\"Operation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***merge the two dataframes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is going to be a little different\n",
    "fips_df = fips_county_df.copy()\n",
    "\n",
    "# add a state column\n",
    "fips_df[\"STATE\"] = fips_df[\"COUNTY_NAME\"]\n",
    "\n",
    "# merge to get the state abbreviations\n",
    "fips_df = fips_df.merge(fips_state_df, on=\"STATE\", how=\"left\")\n",
    "\n",
    "# drop the state column\n",
    "fips_df = fips_df.drop(columns=[\"STATE\"])\n",
    "\n",
    "# rename the columns\n",
    "fips_df = fips_df.rename(\n",
    "    columns={\"FIPS_x\": \"FIPS\", \"ABBREV\": \"STATE\", \"FIPS_y\": \"STATE_FIPS\"}\n",
    ")\n",
    "\n",
    "# forward fill the state fips and state columns\n",
    "fips_df[\"STATE_FIPS\"] = fips_df[\"STATE_FIPS\"].fillna(method=\"ffill\")\n",
    "fips_df[\"STATE\"] = fips_df[\"STATE\"].fillna(method=\"ffill\")\n",
    "\n",
    "# add state to the county name\n",
    "fips_df[\"COUNTY_NAME\"] = fips_df[\"COUNTY_NAME\"] + \", \" + fips_df[\"STATE\"]\n",
    "\n",
    "# filter the df to the states of interest\n",
    "fips_df = fips_df[fips_df[\"STATE\"].isin(states)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sample the FIPS codes to see if we did it right***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fips_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Add some assert tests***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vital_df.all().isnull().sum() == 0\n",
    "# check the first two digits of the FIPS code to make sure they match with STATE_FIPS\n",
    "assert list(fips_df[\"FIPS\"].str[:2]) == list(\n",
    "    fips_df[\"STATE_FIPS\"].astype(str).str.strip()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Save the FIPS file cleaned to the 20_intermediate_files directory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fips_df to a csv\n",
    "fips_df.to_csv(\"../20_intermediate_files/fips_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data\n",
    "\n",
    "This chunk will ingest filtered county population data and return a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ingest data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory path to the raw data\n",
    "census_path = \"../00_source_data/01_census_data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the raw data and filter for states of interest\n",
    "census_df = pd.read_excel(census_path, header=0, skiprows=4, usecols=\"A:B\")\n",
    "\n",
    "# Change the column names to county name and population\n",
    "census_df.columns = [\"COUNTY_NAME\", \"POPULATION\"]\n",
    "\n",
    "# remove the leading period from the county name\n",
    "census_df[\"COUNTY_NAME\"] = census_df[\"COUNTY_NAME\"].str[1:]\n",
    "\n",
    "# move the state name to its own column\n",
    "census_df[\"STATE\"] = census_df[\"COUNTY_NAME\"].str.split(\",\").str[1]\n",
    "census_df[\"STATE\"] = census_df[\"STATE\"].str.strip()\n",
    "census_df[\"STATE\"] = census_df[\"STATE\"].str.upper()\n",
    "\n",
    "# remove the state name from the county name\n",
    "census_df[\"COUNTY_NAME\"] = census_df[\"COUNTY_NAME\"].str.split(\",\").str[0]\n",
    "\n",
    "# change population to an integer\n",
    "census_df[\"POPULATION\"] = census_df[\"POPULATION\"].astype(\"Int64\")\n",
    "\n",
    "# add an abbreviation column for the state from the state_df\n",
    "census_df_merge = census_df.merge(state_df, on=\"STATE\", how=\"outer\")\n",
    "\n",
    "# reformat county name to include abbrev\n",
    "census_df_merge[\"COUNTY_NAME\"] = (\n",
    "    census_df_merge[\"COUNTY_NAME\"] + \", \" + census_df_merge[\"ABBREV\"]\n",
    ")\n",
    "census_df_merge[\"COUNTY_NAME\"] = census_df_merge[\"COUNTY_NAME\"].str.upper()\n",
    "\n",
    "# remove the state column\n",
    "census_df_merge = census_df_merge.drop(columns=\"STATE\")\n",
    "\n",
    "# rename the abbrev column to state\n",
    "census_df_merge = census_df_merge.rename(columns={\"ABBREV\": \"STATE\"})\n",
    "\n",
    "# filter for the states of interest\n",
    "census_df_merge = census_df_merge[census_df_merge[\"STATE\"].isin(states)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Sample to see if we did it right***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census_df_merge.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Add assert tests***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert census_df_merge.all().isnull().sum() == 0\n",
    "# check last two characters of county name to make sure they match with state\n",
    "assert list(census_df_merge[\"COUNTY_NAME\"].str[-2:]) == list(census_df_merge[\"STATE\"])\n",
    "# we need to check county numbers to make sure they match (when we know which control states we want)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Save the output file to the intermediate files directory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file to the intermediate folder\n",
    "census_df_merge.to_csv(\"../20_intermediate_files/census_df.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8252741f7dece6ad0987207e3211c729db738b68c7c87111815d7232d469f1e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
